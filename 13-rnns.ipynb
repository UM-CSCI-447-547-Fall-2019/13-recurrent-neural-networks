{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 13 - Recurrent Neural Networks\n",
    "\n",
    "## [Video Lecture](https://www.youtube.com/watch?v=6niqTuYFZLQ)\n",
    "\n",
    "## 1 The basic recurrent neural network\n",
    "As we've seen, standard Markov Models don't work for capturing long term dependencies when making models of sequences: the number of parameters grows exponentially in the degree of the model (how far we look back in the sequence in order to make predictions about the next thing in the sequence).  This naturally leads us to think about other potential architectures.  Alternatively, we could think about how to use a neural network as a means for language modelling, particularly since we now know how to encode text information, either via one hot encodings or through word embeddings.  For example, we could apply a standard multilayer perceptron to the last $n$ words in a sentence in order to produce the next one.  However, the problem with this is that sentences aren't always the same length: we may have too few inputs (imagine if we tried to predict the third word in a sentence, but our MLP expected ten features), or our fixed input size might be too short (imagine if we tried to predict the 15th word in a sentence, say, a pronoun, but the noun to which it referred was the first word in the sentence).  Multilayer perceptrons are powerful, but ultimately suffer from the baked-in assumption that they *know where the predictive information is stored* (note that this was the same reason that we decided to use CNNs as well).  Recurrent neural networks essentially attempt to circumvent these problems by adopting the sequential structure of a Markov model along with the flexibility of a neural network.\n",
    "\n",
    "The basic equation for determining the internal state of an RNN is\n",
    "$$\n",
    "\\mathbf{h}_t = \\tanh(W_{hh} \\mathbf{h}_t + W_{xh} \\mathbf{x}_t),\n",
    "$$\n",
    "which is to say that it's just the same math as the hidden layer of a multilayer perceptron, but applied *recurrently*: rather than just compute the activation of a linear transformation of the input features $\\mathbf{x}_t$, the activation is applied to the sum of that linear transformation *and* a linear transformation of the *previous state's* activation.  Why is this useful?  It provides context.  Because $\\mathbf{h}_t$ has been built up sequentially by looking at all of the previous states in the sequence, it has the capacity (for judicious parameter choices!) to distill all of that previous information into a representation that is actionable, particularly when combined with the current input.  \n",
    "\n",
    "What is meant by actionable?  As usual $h_t$ is of arbitrary dimension: in order to make predictions, we simply complete the neural network by adding an additional linear transformation to create some outputs:\n",
    "$$\n",
    "\\mathbf{y}_t = W_{hy} \\mathbf{h}_t,\n",
    "$$\n",
    "which could of course be interpreted as logits (to be fed into softmax and cross-entropy) or as reals (to be fed into SSE).  As such, the graph of this RNN (a so-called Elman network, after it's original author) looks like this:\n",
    "<img src=\"elman.svg\" />\n",
    "Just like a normal MLP, except for the linkages from the hidden nodes back to themselves.  For training, the RNN can be *unrolled*, such that we have the situation that\n",
    "<img src=\"unrolled.svg\" />\n",
    "All the boxes represent one application of the hidden input augmented multilayer perceptron, with the additional note that all of the models are identical and share their weights.  Note that in this drawing, all the arrows go up and right: there are no loops, and thus this is a directed acyclic graph for which automatic differentiation works fine.  \n",
    "\n",
    "For concreteness, if we were training a word prediction model, the input $x_t$ would be the encoding of the $t-$th word in the sequence, $y_t$ would be an unscaled log probability (logit) of the next word in the sentence (most likely to be compared to the real next word in a sentence), and $h_t$ represents some internal information distilled from all the inputs that have occurred before.\n",
    "\n",
    "## 2 RNNs don't really work\n",
    "Unfortunately, this specific architecture doesn't work that well in practice.  This is primarily due to the depth of the network, which yields a so-called *vanishing gradient problem*: as misfit information from later positions in the sequence propogates back through the network to interact with input information from earlier, the influence of any particular weight dies out and training fails.  Alternatively, repeated applications of matrix multiplication when weights are large can lead to something called the *exploding gradient problem*, where weights go to infinity via exponentially growing positive gradients.\n",
    "\n",
    "## 3 Long short term memory\n",
    "Long Short Term Memory networks(LSTMs) were designed to have the advantages of RNNs, but without the problematic behavior of exploding and vanishing gradients.  They have become the de facto standard mechanism for sequence modelling (until very recently, when something called a Transformer has supplanted them).  From a very high level view, the figure directly above, where a series of models with shared weights are applied to some input and some internal state in order to produce an output and an updated internal state that is passed to the next model in the sequence, holds.  The only major difference is in the specific operations performed inside that box, which are a little bit more complicated than the basic RNN\n",
    "\n",
    "LSTM work by introducing a mechanism for explicitly remembering and forgetting pieces of information, which are stored in a second internal state called the cell state, which we will call $\\mathbf{c}_t$.  Rather than reproduce the figures here, have a look at [this excellent blog post by Christopher Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)    \n",
    "\n",
    "## 4 Shakespeare simulator 2: Character-level boogaloo\n",
    "While we spent a bit of time thinking about how to encode *words* as numbers in order to input them into a neural network, for understanding the expressive power of LSTMs, it's useful to step back just a bit and do something simpler: character level modelling.  Why should we do this, even though we have the capacity to embed words?  First, it's quite a bit simpler.  The vocabulary of characters is short, just the 26 letters of the alphabet, the various puncuation marks, and the whitespace.  Second, using characters, it's easier to see just how far the memory of this thing has to be in order to produce cogent results: if a word model has to look back several words to determine which pronoun or adjective is appropriate, a character model has to look back perhaps hundreds of characters.  We'll see that this is indeed possible.  \n",
    "\n",
    "First, let's load a corpus of text, just as we did for Markov Models, but also we'll create tokenization dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "#import string\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "sequence_shakespeare = []\n",
    "file = open('t8.shakespeare.txt','r')\n",
    "for line in file:\n",
    "    line.strip('\\n')\n",
    "    if line[:2] == '  ':\n",
    "        line_words = re.findall(r\"[\\w']+|[.,!?;]\",line)\n",
    "        line_words = [str(w).lower() for w in line_words if not w.isupper() and not w.isdigit()] \n",
    "\n",
    "        sequence_shakespeare.extend(line_words)\n",
    "        \n",
    "print (' '.join(sequence_shakespeare[:100]))\n",
    "text = ' '.join(sequence_shakespeare)\n",
    "seq_size = 100\n",
    "texts = []\n",
    "for i in range(len(text)//100):\n",
    "    texts.append(text[i*100:(i*100)+100])\n",
    "\n",
    "# Limit ourselves to the first 100000 characters\n",
    "text = texts[:1000]\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've already batched our sequences: we've produced 1000 sequences each of 100 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, with character level modelling, we have a relatively small vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split our sequences into inputs and targets: **if we are trying to predict the next letter in the sequence, what should our target sequence be?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq.append(text[i][:-1])\n",
    "    target_seq.append(text[i][1:])\n",
    "\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "\n",
    "print(\"Input Sequence: {}\\nTarget Sequence:  {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized input, we can create one hot encodings of our inputs (note that we'll be using categorical cross entropy as a cost function, which in pytorch expects integer labels rather than one-hot encodings: thus we will not one-hot encode our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = len(input_seq[0])\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Create a one hot encoding with batch as the first axis, position in sequence along the second axis,\n",
    "    # and one-hot vector along the third axis\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If possible, we'd like to run this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define our model.  This proceeds much like it did for MLPs and CNNs: we subclass the Module object with an __init__ method and a forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        \"\"\" \n",
    "        Inputs: input_size (vocabulary size)\n",
    "                output_size (also vocabulary size\n",
    "                hidden_dim (size of the hidden state\n",
    "                n_layers (depth of RNN layer)\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "  \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer - (GRU is strongly related to LSTM but works better for smaller datasets)\n",
    "        self.rnn = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True,dropout=0.5)   \n",
    "        \n",
    "        # Fully connected layer - W_hy\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Inputs: x (the sequence)\n",
    "                hidden (the initial hidden layer state)\n",
    "        Outputs: out (logits)\n",
    "                 hidden (the final hidden layer state)\n",
    "                \"\"\"\n",
    "        \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs for input into fully connected layer: essentially this means that \n",
    "        # the batch and the sequence position get flattened into one axis, which is fine because we're \n",
    "        # going to take the sum anyways\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instatiate the model and send it to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=256, n_layers=2)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up our loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "n_epochs = 2000\n",
    "lr=0.002\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll wrap our dataset in a DataLoader to facilitate mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "train = data_utils.TensorDataset(input_seq.cuda(), target_seq.cuda())\n",
    "train_loader = data_utils.DataLoader(train, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we'll want to see the quality of the language predictions as the model proceeds, we'll need to produce a method that creates random strings of characters (according to the model), based on a supplied initial string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, hidden, character, temperature):\n",
    "    # Produce next character as function of current character and hidden state\n",
    "    \n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character).cuda()\n",
    "    \n",
    "    out, hidden = model(character,hidden)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1]/temperature, dim=0).data\n",
    "    char_ind = torch.multinomial(prob, 1).item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "def sample(model, out_len, start='thy',temperature=1.0):\n",
    "    # Create a string of out_len characters by repeated model prediction on the generated sequence.    \n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    hidden = model.init_hidden(1)\n",
    "    for ii in range(size):\n",
    "        char, hidden = predict(model, hidden, chars,temperature)\n",
    "        chars.append(char)\n",
    "    model.train()\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train our model.  This is, of course, highly familiar syntax, with the one exception being that we'll initialize hidden states with the final hidden state from the previous epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Run\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for d,t in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(d,hidden)\n",
    "        loss = criterion(output, t.view(-1).long())\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch%10 == 0 or epoch==1:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "        print(sample(model, 100, 'thy'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(model, 300, 'to',temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
